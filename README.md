# movies_etl

# module 

In this module, we learned how to use the Extract, Transform, Load (ETL) process to create data pipelines. We are able to do the following: 
- Create an ETL pipeline from raw data to a SQL database.
- Extract data from disparate sources using Python.
- Clean and transform data using Pandas.
- Use regular expressions to parse data and to transform text into numbers.
- Load data with PostgreSQL.

Using the three files (ratings, movie_metadata, wiki JSON), we were able to see a lot of information about movies, including budgets and box office returns, cast and crew, production and distribution, and so much more. Using juypter note book, we were able to transform and clean the data. From there, the data was imported the cleaned movies and ratings data to Postgres SQL. 

# Challenge

In this challenge, we wrote a Python script that performs all three ETL steps on the Wikipedia and Kaggle data. We left out any code that performs exploratory data analysis, and you may added a code to handle potentially unforeseen errors due to changes in the underlying data.
